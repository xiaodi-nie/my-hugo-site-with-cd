[{"content":"Ingredients    Item Measure     1/4 inch thick chicken breast 1 1/2 lb   All-purpose flour 3 tbsp   Salt to taste   Pepper to taste   Butter 3 tbsp   Mushrooms 8 oz   Finely chopped shallots 1 medium shallot   Minced garlic 2 cloves   Chicken broth 2/3 cup   Marsala wine 2/3 cup   Heavy cream 2/3 cup   Chopped fresh thyme 2 tsp    Steps  Place flour, salt, pepper in a ziplock bag, add chicken and shake to coat Cook chicken with olive oil and 2 tbsp butter until just barely cooked through, set aside Add 1 tbsp butter to same pan, cook mushrooms Add shallots, garlic and salt, cook a bit Add broth, wine, heavy cream, thyme, salt and pepper, gently boil 10-15 mins(until thickened to about half) Add chicken back, simmer 2-3 mins   ","description":"An amazing italian-american dish that pairs well with either salad, mashed potatoes or rice","id":0,"section":"posts","tags":["savory recipe"],"title":"Chicken Marsala","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/chicken-marsala/"},{"content":"Ingredients Base\n   Item Measure     Almond flour 0.8 cup   Sweetener(swerve) 3 tbsp   Melted butter 3 tbsp   Vanilla extract 2/3 tsp   Eggs 1   Salt small pinch    Filling\n   Item Measure     Full fat cream cheese 16 oz   Sweetener(swerve) 2/3 cup   Eggs 1   Lemon 1/5 to juice   Vanilla extract 2/3 tsp    ~= 6 inch cake\nSteps  Preheat oven to 300F, bring eggs and cream cheese to room temperature  Crust base\n2. Mix dry and wet ingredients seperately, then combine together\n3. lightly grease a 6 inch cake pan and spread batter out in the pan\n4. bake for 15 mins\nCheese filling\n5. Add sweetener to cheese, use hand mixer to combine\n6. Add egg, lemon juice and vanilla extract, combine\n7. Take out baked crust, fill in cheese mixure, bake at 350F for 35 mins\n8. Once it cooled down, cover with tin foil and refrigerate for 5 hours\n ","description":"Low carb lemon-y cheesecake","id":1,"section":"posts","tags":["baking recipe"],"title":"Cheese cake","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/cheesecake/"},{"content":"Ingredients    Item Measure     Almond flour 1 1/2 cup   Baking powder 1 tbsp   Salt 1/4 tsp   Garlic powder some   Onion powder some   Eggs 2   Full fat sour cream 1/2 cup   Melted butter 4 tbsp   Shredded cheddar cheese 1/2 cup    ~= 12 regular muffins\nSteps  Preheat oven to 450F, grease muffin pan Mix together almond flour, salt, baking powder, garlic powder and onion powder In another bowl combine eggs, sour cream, butter, and mix well Mix wet and dry ingredients Add in cheese Add batter in muffin pan and bake for 10-11 mins, until top is light golden brown   ","description":"Low carb savory biscuits with almond flour","id":2,"section":"posts","tags":["baking recipe"],"title":"Almond Flour Biscuits","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/almond-flour-biscuits/"},{"content":"Ingredients    Item Measure     Almond flour 2 1/2 cup   Erythritol(swerve) 1/2 cup   Baking powder 1 1/2 tsp   Salt a small pinch   Melted Butter 1/4 cup   Milk(/oat milk) 1/3 cup   Eggs 3   Vanilla extract 1/2 tsp   Blueberries 3/4 cup    ~= 6 large muffins\nSteps  Preheat oven to 350F, line or grease muffin pan Mix almond flour, sweetener, baking powder and salt Mix in melted butter, milk, eggs, and vanilla extract Add in blueberries Put batter in muffin pan, bake 20-25 min, until top is lightly golden brown.   ","description":"Low carb blueberry muffin with almond flour","id":3,"section":"posts","tags":["dessert recipe"],"title":"Blueberry Muffin","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/blueberry-muffin/"},{"content":"How could ASICs play an important role in Machine Learning going forward? Machine learning tasks, including the training and the inference, usually cost a lot of computational resources and time. So there is a big need for a hardware acceleration. Hardware acceleration is when dedicated hardware, usually in the form of a GPU today, is used to speed up the computing processes present in an AI workflow. Accelerators have many optimisations that make them suitable for training and executing an AI model, and here are some of them:\nParallelism: ML is inherently parallel workload, so it would be one of the biggest requirement for hardware acceleration\nLow-Precision Arithmetic: ML jobs usually works with floating-point numbers, and reducing the precision or ‘detail’ of floating-point operations provides an easy way to increase the effectiveness of the hardware.\nAdvanced Low-Level Architecture: ML-specific chips usually implemented new ways of organizing physical architecture on the chips to achieve better optimization for deep learning applications. An example would be google’s Tensor Processing Unit(TPU). It brings memory and CPU unit closer together to better perform TensorFlow tasks.\nAllowances For In-Memory Processing: The accelerator will also need to make allowances for the smooth execution of in-memory analytics, which speeds up the training by increasing access to the dataset. This includes a high-speed interconnect between the processing unit and the memory (currently HBM2), along with a larger pool of memory to fit big datasets.\nApplication Specific Integrated Circuits, or ASICs, are chips that are designed and manufactured for a specific purpose, task, or application. As opposed to FPGA(Field Programmable Gate Arrays) which are rewritable and reprogrammable, ASICs are permanent and cannot be modified. ASICs do tend to have the best efficiency, performance, as well as power as compared to FPGAs. Although FPGAs are flexible, they can be quite difficult to make and expensive as well. Also they are not as good in terms of performance comparing to GPUs and ASICs. Of course, CPUs can also be used to train and execute ML models, they tend to provide less performance power than optimized hardware chips. GPUs are flexible and fast, their underlying matrix operations and scientific algorithms makes them ideal for graphics. With ASICs, you get the best of all worlds as it is basically a customizable chip that can be designed to accomplish a very specific task at high power, efficiency, and performance.\nReferences https://anysilicon.com/introduction-to-artificial-intelligence-in-asics/\nhttps://analyticsindiamag.com/why-asics-are-becoming-so-widely-popular-for-ai/\n","description":"Cloud Analysis course discussion note on ASICs and machine learning","id":4,"section":"posts","tags":["machine learning","asics"],"title":"Cloud Analysis course discussion note - ASICs","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw6-asics/"},{"content":"How does the CAP Theorem play a role in designing for the cloud? The CAP Theorem says that for a distributed system, you can only have 2 of the 3 things: Consistency, Availability and Partition Tolerance.\nConsistency means that only have one copy of up-to-date data, and that all clients see the same data at the same time, no matter which node they connect to. For this to happen, whenever data is written to one node, it must be instantly forwarded or replicated to all the other nodes in the system before the write is deemed ‘successful.’\nAvailability means data is always available for requests from client even when one or more nodes are down. Another way to state this—all working nodes in the distributed system return a valid response for any request, without exception.\nA partition is basically a network break, or a communication delay/lost between nodes within a distributed system. Partition Tolerance is a system’s ability to still work despite any number of network breakdowns between the nodes.\nAny cloud system is a distributed system, so to design a cloud system we always have to have CAP theorem in mind. Of course, if no partition exists, one system can have both consistency and availability, but there’s always going to be times when things go wrong, so we have to make a choice between C and A. what’s the ultimate design goal in a distributed system for maximum data consistency and 100% availability – it’s design for failure.\nWhat are the implications of Amdahl’s law for Machine Learning projects? Amdahl’s law gives the following formula to measure the speedup of running sub-tasks in parallel (over different processors) versus running them sequentially (on a single processor):\n$$ S_L (s) = \\cfrac{1}{(1-p) + \\cfrac{p}{s}} $$\nwhere s is the speedup of the part of the task that benefits from improved system resources, and p is the proportion of execution time that the part benefiting from improved resources originally occupied(parallel portion). So the theoretical time benefit from doing things parallel is largely dependent on how much of the program is made parallel, which is p.\nEven you can achieve perfect parallelism and scale up the program to match the theoretical estimation, the more cores you have, the less it’s going to improve the performance. There’s always overhead with parallelism at the hardware level(additional computer cycles required to divide tasks into subtasks and compile results after). It’s going to grow the more you parallelize, and it also applies for parallelism in a cloud environment.\nIn conclusion, even though it’s tempting to parallelize large data set in machine learning project, you should always keep in mind that parallelism wouldn’t necessarily lead to performance improvement.\nReferences https://www.ibm.com/cloud/learn/cap-theorem\nhttps://en.wikipedia.org/wiki/Amdahl%27s_law\nhttps://www.kdnuggets.com/2017/04/must-know-parallelism-algorithms.html\n","description":"Cloud Analysis course discussion note on CAP Theorem and Amdahl's Law, and their relations with machine learning projects","id":5,"section":"posts","tags":["cloud computing","machine learning"],"title":"Cloud Analysis course notes - CAP Theorem and Amdahl's Law","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw5-cap/"},{"content":"What are containers? The concept of containers is similar to that of virtual machines. The differentiating factor is that VMs virtualize at the hardware level and containers virtualize at the operating system level. The containerization approach creates a more lightweight and flexible environment by allowing applications to share an operating system while maintaining their own executables, code, libraries, tools, and configuration files.\nA container consists of an entire runtime environment: an application, plus all its dependencies, libraries and other binaries, and configuration files needed to run it, bundled into one package. By containerizing the application platform and its dependencies, differences in OS distributions and underlying infrastructure are abstracted away.\nWhat problem do containers solve? Let’s look at this problem by comparing containers with VMs. Comparing to VMs, containers offers the following benefits:\n Lightweight  A container may be only tens of megabytes in size, whereas a virtual machine with its own entire operating system may be several gigabytes in size. Because of this, a single server can host far more containers than virtual machines.\n Just-in-time  Virtual machines may take several minutes to boot up their operating systems and begin running the applications they host, while containerized applications can be started almost instantly. That means containers can be instantiated in a \u0026ldquo;just in time\u0026rdquo; fashion when they are needed and can disappear when they are no longer required, freeing up resources on their hosts.\n Modularity  Rather than run an entire complex application inside a single container, the application can be split in to modules (such as the database, the application front end, and so on). This is the so-called microservices approach. Applications built in this way are easier to manage because each module is relatively simple, and changes can be made to modules without having to rebuild the entire application.\n Scalable  Since they have all of the above advantages, containers can be easily scaled up. You can basically run 1000 environments at the same time at a click of your fingers.\n","description":"Cloud Analysis course discussion note on containers","id":6,"section":"posts","tags":["cloud computing","containers"],"title":"Cloud Analysis course notes - Containers","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw4-containers/"},{"content":"What are the different layers of network security on AWS and what unique problems do each solve? According to the AWS security white paper, AWS has the following layers of network security:\n Secure Network Architecture  There are network devices like firewalls at the edge of the aws network to closely monitor and guard the information flow in both ways. ACLs, or Access Control List are in place to manage the traffic flow.\n Secure Access Points  AWS has a limited number of access points(API endpoints) to the cloud to monitor more comprehensively of inbound and outbound traffic. They allow HTTPS access which we can use to establish secure connection with our storage or compute instances within AWS.\n Transmission Protection  SSL protocal(Secure Sockets Layer) can be utilized to connect to AWS access point and prevent eavesdropping, tampering and message forgery. In addition, Amazon Virtual Private Cloud (VPC) offers a private subnet within the AWS cloud, and VPN can also be used to establish an encrypted tunnel between the Amazon VPC and your data center.\n Amazon Corporate Segregation  The AWS Production network is segregated from the Amazon Corporate network. Only approved personnel, developers and administrators, can access the AWS corporate network through a bastion host that restricts access to cloud components which requires a SSH key, and all activities will be logged for security review.\n Fault-Tolerant Design  Amazon’s infrastructure has a high level of availability. Data centers are built in clusters in various global regions. If failure happens, customer data will be automatically moved away from the affected area. We can also place instances and store data within multiple geographic regions/availability zones to provide more physical separation and better availability.\n Network Monitoring and Protection  There are a wide variety of automated monitoring systems to detect unusual or unauthorized activities and conditions at incoming and outgoing communication points. Alarms are configured to notify operations and management personnel when thresholds are crossed on key operational metrics. On top of that, there are always people on call to handle those alarms or issues regarding operation. Ticketing system is in place to propagate severe/emergence incident to upper management. There are also documentation of aid and help personnel to handle incidents.\nWhat problem do AWS Spot instances solve and how could you use them in your projects? One of the problems with the on-demand business model (like normal EC2 instances) is that at any given time, there are likely to be compute resources that are not being utilized. These resources represent hardware capacity that AWS has paid for but are sitting idle. Rather than allowing these computing resources to go to waste, AWS offers them at a substantially discounted rate, with the understanding that if someone needs those resources for running a normal EC2 instance, that instance will take priority over spot instances and the spot instance may be interrupted at any time.\nThose who wish to use spot resources simply tell AWS how much they are willing to pay per hour for those resources. Spot instances will continue to run until the cost of the resource meets or exceeds the agreed-upon price. Of course, subscribers are also free to stop or terminate spot instances themselves.\nSince spot instances are subject to sudden interruptions, they should not be used on situations where your applications need to be up and running at all times.\nHowever, in our project, if we want to do an experiment or prototype, anything temporary, we can request a spot instance and utilize large computational power at a much lower price.\n","description":"Cloud Analysis course discussion note on AWS network security strategy and spot instance","id":7,"section":"posts","tags":["cloud computing","aws"],"title":"Cloud Analysis course notes - AWS","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw3-aws/"},{"content":"What is IAC and what problem does it solve? IAC stands for Infrastructure as Code. is a method to provision and manage IT infrastructure through the use of source code, rather than through standard operating procedures and manual processes.\nYou’re basically treating your servers, databases, networks, and other infrastructure like software. And this code can help you configure and deploy these infrastructure components quickly and consistently.\nIaC helps you automate the infrastructure deployment process in a repeatable, consistent manner, which has many benefits.\nWith IaC, deployment doesn’t always have to all fall on DevOps’ shoulders. Developers today can write not only application code, but also the infrastructure to run applications. And companies today can even integrate infrastructure code into the software development system to vastly increase development efficiency.\nIaC let machines do all the work to set up cloud environment and manage resources that you need, which makes it faster and more efficient than humans would do.\nIt also makes continuously testing systems and processes possible for infrastructure configuration. In modern software systems, you can build a \u0026ldquo;deployment pipeline\u0026rdquo; for infrastructure code, which allows you to practice continuous delivery processes for infrastructure changes.\nIf infrastructure is being turned into code, the biggest perk should be version control. With version control it would be easier to roll back to a working configuration if anything goes wrong, and the environment would be easier to track and maintain, errors can be found and fixed more quickly.\nHow should a company decide on what level of cloud abstraction to use SaaS Software as a Service. SaaS utilizes the internet to deliver applications, which are managed by a third-party vendor, to its users. A majority of SaaS applications run directly through your web browser, which means they do not require any downloads or installations on the client side. Use it if you are after features that you don’t want to build yourself and that can significantly improve your product\nPaaS Platform as a Service. Example: Google Kubernetes Engine, Azure Container Service. PaaS delivers a framework for developers that they can build upon and use to create customized applications.Use it if you want to just focus on the application code without having to worry about infrastructure.\nIaaS Infrastructure as a Service. Example: AWS EC2, AWS VPC, AWS ELB. IaaS is fully self-service for accessing and monitoring computers, networking, storage, and other services. IaaS allows businesses to purchase resources on-demand and as-needed instead of having to buy hardware outright. Use it if you want granular control over your product’s infrastructure.\nMaaS Metal(Machine) as a Service. Use if you want just the machine and the freedom to install OS and do configurations yourself. This is particularly useful for multi-nodes applications like Hadoop clusters.\nServerless It’s basically FaaS(Function as a Service, like AWS Lambda)+BaaS(Backend as a Service, like AWS DynamoDB, Google App Engine). It’s similar to Paas, use it if you want to focus on code, but Serverless provides even more deployment ease and even less configuration.\n","description":"Cloud Analysis course discussion note on IAC, SaaS, PaaS, IaaS, MaaS and Serverless","id":8,"section":"posts","tags":["cloud computing","IaC"],"title":"Cloud Analysis course note - IaC and cloud service model","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw2-iac/"},{"content":"This is the basic workflow of this blog and list of steps to continuous deployment. It is also a course note and the majority of the materials came from my professor\u0026rsquo;s post here\nSet up Hugo workflow using AWS Cloud9 and Github First we are going to setup a cloud9 environment as the working directory of the whole thing.\nCloud9 provides a quick and easy linux environment/web-based editor hosted on an EC2 instance that would be automatically stopped after 30m of unuse.\nIt\u0026rsquo;s quite cost effective and convenient especially for windows users.\n  Launch an AWS Cloud9 Environment   Log into your AWS account and go to the Cloud9 console. Create a new environment with the free tier instance and the default configurations.\n  Install Hugo   Use wget to downlad the latest version of hugo binary. You can find the latest release here.\nwget https://github.com/gohugoio/hugo/releases/download/v\u0026lt;LATEST_VERSION\u0026gt;/hugo_\u0026lt;LATEST_VERSION\u0026gt;_extended_Linux-32bit.tar.gz Note that here we used the extended version since we are going to use a theme that depends on the extended version of hugo.\nNow unzip and install the hugo binary:\ntar xzvf hugo_0.64.0_extended_Linux-32bit.tar.gz mkdir -p ~/bin mv ~/environment/hugo ~/bin #assuming that you download this into ~/environment which hugo #this shows the `path` to hugo and should be '~/bin/hugo' hugo version #check that hugo is successfully installed   Create a website   You might have seen a lot of people saying that hugo is better and faster than jekyll, and part of the reason is that hugo is just a go binary.\nBecause of that, development and deployment is very simple and quick with hugo.\nNow you can use hugo new site mywebsite to create a new site.\nHugo has a wide range of theme selections and in my personal experience the best way to use themes is:\n Fork theme repo into your github account  This blog used hugo-theme-zzo\nAdd the theme into your website as a submodule  cd mywebsite git submodule add https://github.com/\u0026lt;YOUR_GITHUB_ID\u0026gt;/hugo-theme-zzo themes/zzo Make your own changes to the theme and track it into your forked repo Update the main site repo to the latest theme repo version(we will be tackling this later) Add the theme into the config file  echo 'theme = \u0026quot;zzo\u0026quot;' \u0026gt;\u0026gt; config.toml\ngit pull changed and update from the original theme repo if necessary(may need to fix conflict)  Now create a new post:\nhugo new posts/my-first-post.md\nThis will create a new markdown file with the created timestamp. You can edit this file with the cloud9 editor.\nNote that if you use this command to generate a new post, the draft field will be defaulted to true which means this file will not be built into html and it will not be visible on the deployed website.\n  Run Hugo locally in Cloud9   Here we are going to run hugo as a deployment server. To access the port after the server has started, we will need to open up a port on the EC2 security groups.\nJust look for the security group tab on the EC2 console page, and look for the one with the same name as your current Cloud9 environment:\nGo to the Inbound tab, click the Edit button and add a new Custom TCP Rule with port 8080.\nNow we can go back to Cloud9 and use curl ipinfo.io to find out the public ip address. The output should look something like this:\n1 2 3 4 5 6 7 8 9 10 11 12  { \u0026#34;ip\u0026#34;: \u0026#34;54.80.213.10\u0026#34;, #this is the address we want \u0026#34;hostname\u0026#34;: \u0026#34;ec2-54-80-213-10.compute-1.amazonaws.com\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Virginia Beach\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;Virginia\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;loc\u0026#34;: \u0026#34;36.8512,-76.1692\u0026#34;, \u0026#34;org\u0026#34;: \u0026#34;AS14618 Amazon.com, Inc.\u0026#34;, \u0026#34;postal\u0026#34;: \u0026#34;23465\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;America/New_York\u0026#34;, \u0026#34;readme\u0026#34;: \u0026#34;https://ipinfo.io/missingauth\u0026#34; }   Run hugo with the following options. This will start the server and left it running.\nhugo serve --bind=0.0.0.0 --port=8080 --baseURL=http://\u0026lt;YOUR_PUBLIC_IP_ADDRESS\u0026gt;/\nOpen up a new tab in your browser and go to http://\u0026lt;YOUR_PUBLIC_IP_ADDRESS\u0026gt;:8080/ to preview your site. Cool thing about this is you can edit your markdown file and it will render and reflect the change on the site in real time.\nHost the static site on S3 First follow this instruction to set up an S3 bucket for static website hosting.\nYou should also set a bucket policy as below from the Permissions tab on your bucket page, with your own bucket name:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;YOUR_BUCKET_NAME\u0026gt;/*\u0026#34; ] } ] }     Deploy website manually to S3   Before going fully automatic, it\u0026rsquo;s best to first deploy to your bucket manually and see if everything works out.\nAdd the following to the existing config.toml file(overwrite fields if exists)\n1 2 3 4 5 6 7 8 9  baseURL = \u0026#34;http://\u0026lt;YOUR_BUCKET_NAME\u0026gt;.s3-website-\u0026lt;YOUR_BUCKET_REGION\u0026gt;.amazonaws.com\u0026#34; title = \u0026#34;\u0026lt;YOUR_TITLE\u0026gt;\u0026#34; theme = \u0026#34;zzo\u0026#34; [[deployment.targets]] # An arbitrary name for this target. name = \u0026#34;\u0026lt;WHATEVER_NAME\u0026gt;\u0026#34; URL = \u0026#34;s3://\u0026lt;YOUR_BUCKET_NAME\u0026gt;/?region=\u0026lt;YOUR_BUCKET_REGION\u0026gt;\u0026#34; #your bucket here   Now you can deploy using the following commands:\nhugo hugo deploy If everything works out you should see something like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ec2-user:~/environment/quickstart (master) $ hugo | EN -------------------+------ Pages | 34 Paginator pages | 0 Non-page files | 0 Static files | 105 Processed images | 0 Aliases | 9 Sitemaps | 1 Cleaned | 0 Total in 5692 ms ec2-user:~/environment/quickstart (master) $ hugo deploy Deploying to target \u0026#34;jan23awsbucket\u0026#34; (s3://jan23-website-hosting/?region=us-east-1) Identified 1 file(s) to upload, totaling 244 kB, and 0 file(s) to delete. Success!   Now your website should be accessible through the static website hosting endpoint.\nContinuous Delivery with AWS CodeBuild   Check code into Github    Create a new repo and add .gitignore with Go (Optional) Add public to .gitignore to skip pushing all built html files to repo Create a Makefile to remove the public folder:  1 2 3  clean: echo \u0026#34;deleting generated html...\u0026#34; rm -rf public   Now run make clean to remove all the generated html files to prevent them from being uploaded to version control.\nAdd repo as a remote and push code  git remote add origin git@github.com:\u0026lt;github_username\u0026gt;/my_hugo_site.git git status git add * git pull --allow-unrelated-histories origin master git branch --set-upstream-to=origin/master git push   Setup project on AWS CodeBuild   Go to AWS CodeBuild and create a new project in the same region as your S3 bucket\nNote in the Source section, use OAuth to login to Github and choose Repository in my Github Account. In Additional Configuration, choose git clone depth = 1, tick Use Git submodules and Report build statuses to source provider when your builds start and finish\nIn the Webhook section, tick Rebuild every time a code change is pushed to this repository.\nThe Environment section should look something like this:\nNow go to the Service role page from the project detail page, and attach an AdministratorAccess policy to that role.\nOk, one last step. Go back to Cloud9 and add a buildspec.yml file to tell AWS what you want to do with the automatic build:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  version:0.2environment_variables:plaintext:HUGO_VERSION:\u0026#34;0.64.0\u0026#34;phases:install:runtime-versions:docker:18commands:- cd/tmp- wgethttps://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_${HUGO_VERSION}_extended_Linux-64bit.tar.gz- tar-xzfhugo_${HUGO_VERSION}_extended_Linux-64bit.tar.gz- mvhugo/usr/bin/hugo- cd- - rm-rf/tmp/*build:commands:- rm-rfpublic- hugo- awss3syncpublic/s3://\u0026lt;YOUR_BUCKET_NAME\u0026gt;/--region\u0026lt;YOUR_BUCKET_REGION\u0026gt;--deletepost_build:commands:- echoBuildcompletedon`date`  DONE! Now you can push something to your github repo and check out the new build that pops up on the build history page of your CodeBuild project.\n","description":"Steps and notes to build a continuous deployment workflow of Hugo website with the help of AWS S3 and CodeBuild","id":9,"section":"posts","tags":["cloud computing","hugo","aws","continuous deployment"],"title":"Hosting Hugo Website on AWS S3 with Continuous Deployment","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hugo-on-s3-with-cd/"},{"content":"The git revert command can be considered an \u0026lsquo;undo\u0026rsquo; type command, however, it is not a traditional undo operation. Instead of removing the commit from the project history, it figures out how to invert the changes introduced by the commit and appends a new commit with the resulting inverse content. This prevents Git from losing history, which is important for the integrity of your revision history and for reliable collaboration.\nSyntax git revert \u0026lt;COMMIT_ID\u0026gt; Here COMMIT_ID can be any form that git recognizes, from regular 40-digit, short SHA-1, to something like \u0026lsquo;HEAD\u0026rsquo;.\nThis means any change done by \u0026lt;COMMIT_ID\u0026gt; will be reverted and a new commit is created. You will be prompted to enter a regular commit message and after that you can do git push normally.\ngit revert \u0026lt;COMMIT_ID\u0026gt;..HEAD Reverts all the changes done by the commits in between, or similarly you can also do\ngit revert \u0026lt;COMMIT_ID1\u0026gt;..\u0026lt;COMMIT_ID2\u0026gt; Compare to other \u0026lsquo;undo\u0026rsquo; methods git reset This is like a complete rollback and it would move the working tree back to the last commited state. No new commit will be created and commits after the designated target may be lost. And it will change the commit history.\nYou can have several options as to what to do with your index(set of files that\u0026rsquo;s going to be the next commit) and the working directory:\ngit reset --soft will only change the current HEAD to point to another commit, but the index and working directory remains unchanged.\ngit reset --mixed will change the HEAD pointer and the index, but the working directory will stay the same.\ngit reset --hard will change everything. This means every change after the other commit will be gone forever.\n(However if you accidentally erased something with the \u0026ndash;hard option there\u0026rsquo;s always git reflog to help you get almost everything back)\ngit checkout This is used to change the HEAD pointer to another commit or switch between branches. It will also rollback file changes after that commit and can change the files in the working directory. The commit history will not be changed.\nTypical use cases:\n undo local changes to a file: git checkout file_name undo local commits:  git checkout target_branch git reset HEAD~2 #rollback 2 commits Git cheat sheet ","description":"Git revert and its comparison to reset","id":10,"section":"posts","tags":["git"],"title":"Git Revert","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/git-revert/"},{"content":"This is a course note and the majority of the meterials came from this qwiklab\nOverview Google Kubernetes Engine (GKE) provides a managed environment for deploying, managing, and scaling your containerized applications using Google infrastructure. The Kubernetes Engine environment consists of multiple machines (specifically Google Compute Engine instances) grouped together to form a container cluster.\nKubernetes Engine clusters are powered by the Kubernetes open source cluster management system. Kubernetes provides the mechanisms through which you interact with your container cluster. You use Kubernetes commands and resources to deploy and manage your applications, perform administration tasks and set policies, and monitor the health of your deployed workloads.\nKey components and concepts Pods Pods represent and hold a collection of one or more containers. Generally, if you have multiple containers with a hard dependency on each other, you package the containers inside a single pod.\nPods also have volumes attached to them and containers within a pod can share this attached volume through a shared namespace and communicate with each other.\nOne pod had one shared network namespace which means there\u0026rsquo;s one ip address per pod.\nServices Services provide stable endpoints for Pods. They are there to help you communicate with a set of pods, since pods don\u0026rsquo;t tend to be persistent and they sometimes would stop or restart due to reasons like failed liveness or readiness checks, and that would result in different ip addresses.\nServices used labels to point to pods. So all you have to do to add endpoints to a service is giving a label to each pod inside and the service will pick them up and expose them.\nThere are three types of services:\n ClusterIP (internal) \u0026ndash; the default type means that this Service is only visible inside of the cluster NodePort \u0026ndash; gives each node in the cluster an externally accessible IP LoadBalancer \u0026ndash; adds a load balancer from the cloud provider which forwards traffic from the service to Nodes within it.  Deployments Deployments are there to make sure that there are always that amount of pods(replicas) that you want there to be. Deployments use Replica Sets to manage starting and stopping the Pods.\nAnother cool thing about deployments is that it can recreate and start pods on another node automatically if the node that some pods are residing suddenly failed, thus make the pods inside highly available.\nkubectl commands List of commands and example usage can be found on the above qwiklab.\n","description":"Kubernetes and its orchestration basics","id":11,"section":"posts","tags":["cloud computing","containers"],"title":"Kubernetes basics","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/kubernetes/"},{"content":"Nutty Cloud Souffle this is the recipe for my easy souffle\n","description":"","id":14,"section":"posts","tags":null,"title":"Another Post","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/another-post/"},{"content":"Test on continuous delivery with aws codebuild this is an update lalala\n","description":"","id":16,"section":"posts","tags":null,"title":"Test Post on CodeBuild","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/my-first-post/"}]