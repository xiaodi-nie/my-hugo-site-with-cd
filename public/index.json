[{"content":"Cloud Functions on GCP allows developers to build lightweight API quickly that scales instantly. The function can be triggered by event or invoked over HTTP/S. In this post we will be building a simple REST API that returns the first sentence of any wikipedia entry in any specified language.\nSteps Create a Cloud Function from GCP console\n Go to the GCP console and find Cloud Functions, click Create Function Give it a name, here we\u0026rsquo;ll leave the trigger as HTTP Tick Allow unauthenticated invocations for the API to be public. We set this now for simple testing purposes, if you want your API not to be public, you can untick this box and use IAM to set specific permissions after the function has been created. Here we\u0026rsquo;ll use the inline editor to put in the source code. Cloud Functions allows you to use requirements.txt to install python packages. Just change the runtime to Python 3.7 and put in the function code and package names  Here we used the wikipedia library to get the info and google translate library to translate that info into any language. Don\u0026rsquo;t forget to put the function name into the Function to execute box to make sure the right code is executed when you invoke the API.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  import wikipedia from google.cloud import translate def sample_translate_text(text=\u0026#34;YOUR_TEXT_TO_TRANSLATE\u0026#34;, project_id=\u0026#34;YOUR_PROJECT_ID\u0026#34;, language=\u0026#34;fr\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Translating Text.\u0026#34;\u0026#34;\u0026#34; client = translate.TranslationServiceClient() parent = client.location_path(project_id, \u0026#34;global\u0026#34;) # Detail on supported types can be found here: # https://cloud.google.com/translate/docs/supported-formats response = client.translate_text( parent=parent, contents=[text], mime_type=\u0026#34;text/plain\u0026#34;, # mime types: text/plain, text/html source_language_code=\u0026#34;en-US\u0026#34;, target_language_code=language, ) print(f\u0026#34;You passed in this language {language}\u0026#34;) # Display the translation for each input text provided for translation in response.translations: print(u\u0026#34;Translated text: {}\u0026#34;.format(translation.translated_text)) return u\u0026#34;Translated text: {}\u0026#34;.format(translation.translated_text) def translate_test(request): \u0026#34;\u0026#34;\u0026#34;Takes JSON Payload {\u0026#34;entity\u0026#34;: \u0026#34;google\u0026#34;}\u0026#34;\u0026#34;\u0026#34; request_json = request.get_json() if request_json and \u0026#39;entity\u0026#39; in request_json: entity = request_json[\u0026#39;entity\u0026#39;] language = request_json[\u0026#39;language\u0026#39;] print(entity) res = wikipedia.summary(entity, sentences=1) trans=sample_translate_text(text=res, project_id=\u0026#34;cloud-function-demo-272320\u0026#34;, language=language ) return trans else: return f\u0026#39;No Payload\u0026#39;   Here the Function to execute is translate_test\n1 2 3 4  # Function dependencies, for example: # package\u0026gt;=version wikipedia google-cloud-translate   Invoke the API\n  After the Cloud Function is created, go to the detail page and find the URL under the Trigger tab.\n  You can now send an HTTP request to call the API. For example you can use curl like this:\n  curl -H \u0026quot;Content-Type: application/json\u0026quot; -X POST -d '{\u0026quot;entity\u0026quot;:\u0026quot;facebook\u0026quot;,\u0026quot;language\u0026quot;: \u0026quot;zh\u0026quot;}' \u0026lt;YOUR_URL\u0026gt; The above command would send a JSON payload requesting the first sentence of the wikipedia entry of facebook and translate it into Chinese. It will return something like this:\nTranslated text: Facebook是位于加利福尼亚州门洛帕克的美国在线社交媒体和社交网络服务，也是同名公司Facebook，Inc.的旗舰服务。 ","description":"Steps and notes to build an API quickly using Google Cloud Functions","id":0,"section":"posts","tags":["serverless","google cloud"],"title":"Build your own API with Cloud Functions on GCP","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/cloud-api/"},{"content":"The code was largely taken from this repo. However, the original code couldn\u0026rsquo;t handle unicode inupt and couldn\u0026rsquo;t clean out punctuations. I tweaked the code a little bit to prepocess the input file so that only lower case words without punctuations will be counted.\nSteps Create Cluster\n  Log in to your AWS console and go to EMR to create a spark cluster with the following configurataions:\n  Don\u0026rsquo;t forget to choose or create a EC2 key pair so that we can ssh onto the master node later:\n  It will take around 15 minutes for the cluster to spin up. You should see the cluster status show Ready when it\u0026rsquo;s fully up and running.\nAdd inbound rule on master node\n On your console page go to Services -\u0026gt; EC2 From the left panel find Security Group Select the security group named ElasticMapReduce-master and edit its inbound rule from the Inbound Rules tab Add a new rule with SSH as the Type and ip of your machine that is going to be used to ssh onto the cluster as the Source  Upload input file to S3\n Go to Services -\u0026gt; S3 and create a new bucket, untick block all public access to make the objects inside public to read Create a folder in the bucket and upload the RomeoAndJuliet.txt file from this repo. Or any text file you want to count the words from.  Create code file on master node\n Go back to EMR, and select the cluster you just created Near the \u0026ldquo;Master public DNS:\u0026rdquo; field click the SSH button and SSH on the master node with the platform of your choice In /home/hadoop create wordcount.py (vi wordcount.py) Copy over the contents from wordcount.py in this repo Don\u0026rsquo;t forget to change the input file s3 url in the code to point to the text file in your bucket  Run the spark application\n Still on the master node, execute the script using spark-submit wordcount.py | tee output.txt   You can view the logs and printed result of the word count application in output.txt\n (optional)You can have the output file copied to your s3 bucket by using aws s3 cp output.txt s3://my_bucket/my_folder/   Terminate the cluster\n Don\u0026rsquo;t forget to terminate the cluster after you are done(EMR clusters will keep your bills going up even if it\u0026rsquo;s not doing anything) Next time you need a cluster just create a new one and reuse the same key pair  Here\u0026rsquo;s the source code\n","description":"A step-by-step of how to run a simple map reduce spark application on an Amazon EMR cluster","id":1,"section":"posts","tags":["map reduce","aws","spark"],"title":"Simple Spark Application on Amazon EMR","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/wordcount-on-emr/"},{"content":"What are the tradeoffs with a serverless architecture? There are many advantages of using a serverless architecture, and to name a few:\nLower operation cost: You don’t have to set up infrastructure and can pay for only what you use.\nLess time to market: The serverless architecture style motivates to use existing or third party services. Because functions are small and have a clear responsibility the likelihood for reusing existing functions increases. The platforms to operate serverless architectures takes care of all the operating stuff. The developers just have to upload the code and get direct feedback how the code behave in production. Thus overall the developing time can be shortened comparing to other architecture and the product can make it to market much faster.\nElasticity: Going serverless means the system has the built-in ability of scaling up and down automatically according to the load need\nHowever, there are also a number of trade-offs to consider when going the serverless route:\nSystemic complexity: serverless architecture tend to produce \u0026ldquo;nanoservices\u0026rdquo; and requires more experienced engineers to get it right. By definition “nanoservice” is an antipattern where a service is too fine-grained. A nanoservice is a service whose overhead (communications, maintenance, and so on) outweighs its utility.\nLock-in: To run a system on one vendor you must use very specific services provided by the vendor (for example to compliment Amazon Lambda you have to use Amazon API Gateway, DynamoDB, S3, etc.). Once you developed a complex system on top of these services you are cursed to stick with them, no matter how often they decide to increase their prices. And migration to another lower-priced vendor may have higher overhead than your actual savings.\nResource limitations: Functions have limitations in regards to memory allocation, timeout, payload sizes, deployment sizes, concurrent executions, etc.\nDevelopment difficulties: integration test and debugging can be difficult. You may not have control over all the other services surrounding your function, and debugging from vendor platform may not be so convenient.\nWhat are the advantages of developing with Cloud9? Since AWS Cloud 9 is cloud-based, users are encouraged to have a flexible and efficient development environment. Developers can run, write, and debug apps through a web browser that frees them from the usual local IDE constraints.\nWith AWS Cloud9’s collaboration features, teams can work and code together online and in real time. Users can share their own development workplace with other teammates through a few simple clicks and even do a pair-program while working.\nCloud9 pre-configures the user’s development environment with all the essential libraries, plug-ins, and SDKs it needs for a serverless development. And it also provides an environment for debugging the AWS Lambda functions and local testing.\nMoreover, AWS Cloud9 IDE has a terminal with sudo privileges connected to the managed Amazon EC2 instance that hosts the whole development environment. The setup also has a pre-authenticated AWS Command Line Interface. With these features, users can access AWS services and run commands quicker.\nReferences https://medium.com/@pablo.iorio/serverless-architectures-i-iii-design-and-technical-trade-offs-8ca5d637f98e\nhttps://specify.io/concepts/serverless-architecture\n","description":"course notes on serverless architectures.","id":2,"section":"posts","tags":["serverless","aws"],"title":"Cloud Analysis Course Notes - Serverless Architecture","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw9-lambda/"},{"content":"Here are some notes about eslint and its rule configuration I learned along the way:\nThis is what I have in the package.json configuration file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  \u0026#34;eslintConfig\u0026#34;: { \u0026#34;root\u0026#34;: true, \u0026#34;env\u0026#34;: { \u0026#34;node\u0026#34;: true }, \u0026#34;extends\u0026#34;: [ \u0026#34;plugin:vue/base\u0026#34;, \u0026#34;eslint:recommended\u0026#34; ], \u0026#34;parserOptions\u0026#34;: { \u0026#34;parser\u0026#34;: \u0026#34;babel-eslint\u0026#34; }, \u0026#34;rules\u0026#34;: { \u0026#34;no-console\u0026#34;: 0, \u0026#34;no-unused-vars\u0026#34;: 0, \u0026#34;no-debugger\u0026#34;:0 } },   Since I\u0026rsquo;m still learning and half way through the code there may be some unused variables or components, I turned no-unused-var off. For debugging purposes I also turned no-debugger and no-console off so that I can set breakpoints and log messages during development.\nAlso the default configuration generated using vue-cli had \u0026quot;plugin:vue/essential\u0026quot; in the extends field where it enforces error prevention and enables the vue/no-unused-component rule. I changed it to \u0026quot;plugin:vue/base\u0026quot; to get rid of the error everytime I registered but haven\u0026rsquo;t got around to use the component.\nI Know these are bad practices and you can never be too strict on increasing the readability of the code. However this is just to note down what I did during development for future references.\nreference:\nhttps://eslint.vuejs.org/rules/\n","description":"learnings about eslint plugin configurations","id":3,"section":"posts","tags":["vue"],"title":"Vue Eslint Config","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/vue-package-config/"},{"content":"I recently started learning vue and here is a little note on what would happen in each stage of the vue lifecycle.\nLifecycle Diagram This is a diagram explaining the details from vue.js official docs:\nProject setup Here we have a basic helloWorld prject generated using vue-cli. In our App component there are two variables, name and show.\nIn App.vue we have a template:\n1 2 3 4 5 6 7 8  \u0026lt;template\u0026gt; \u0026lt;div id=\u0026#34;app\u0026#34;\u0026gt; \u0026lt;img alt=\u0026#34;Vue logo\u0026#34; src=\u0026#34;./assets/logo.png\u0026#34;\u0026gt; \u0026lt;p\u0026gt;{{name}}\u0026lt;/p\u0026gt; \u0026lt;button @click=\u0026#34;update\u0026#34;\u0026gt;update data\u0026lt;/button\u0026gt; \u0026lt;HelloWorld msg=\u0026#34;Welcome to Your Vue.js App\u0026#34; v-if=\u0026#34;show\u0026#34;/\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/template\u0026gt;   In main.js, $mount(el) is called:\n1 2 3 4  new Vue({ render: h =\u0026gt; h(App), }).$mount(\u0026#39;#app\u0026#39;)   Lifecycle hooks There are several hooks that will be called before and after each stage. You can put some code inside if you want to do things at some stage. These hooks are beforeCreate(),created(),beforeMount(),mounted(), beforeUpdate(), updated(), beforeDestroy(), destroyed() respectively.\nIn our source code, data and el are output to the console in each hook method.\n  beforeCreate(), created()\ndata and el are both undefined when beforeCreate() is called.\ndata is initialized when created() is called, At this stage, the instance has finished processing the options which means the following have been set up: data observation, computed properties, methods, watch/event callbacks. However, the mounting phase has not been started, and the $el property will not be available yet.\n  beforeMount(), mounted()\nOnly after mounted() is called will el be created and initialized. And now, value of el is the whole div with id=\u0026quot;app\u0026quot;.\n  beforeUpdate(), updated()\nbeforeUpdate() is called when data changes, before the DOM is patched. This is a good place to access the existing DOM before an update, e.g. to remove manually added event listeners.\nupdated() is called after a data change causes the virtual DOM to be re-rendered and patched.\n  beforeDestroy(), destroyed()\nThese hooks will only be called when a vue instance is destroyed and no longer exists in the DOM. Here we use a v-if to destroy the helloWorld component instance to invoke these hooks in the helloWorld.vue component source code.\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14  \u0026lt;script\u0026gt; export default { name: \u0026#39;HelloWorld\u0026#39;, props: { msg: String }, beforeDestroy(){ console.log(\u0026#39;before destroy\u0026#39;) }, destroyed(){ console.log(\u0026#39;destroyed\u0026#39;) }, } \u0026lt;/script\u0026gt;   In our template there\u0026rsquo;s a button and a method called update will be invoked on click. In this method, boolean variable show will be updated from its initial value true to false, which will then make HelloWorld invisible.\n1 2  \u0026lt;button @click=\u0026#34;update\u0026#34;\u0026gt;update data\u0026lt;/button\u0026gt; \u0026lt;HelloWorld msg=\u0026#34;Welcome to Your Vue.js App\u0026#34; v-if=\u0026#34;show\u0026#34;/\u0026gt;   1 2 3 4 5 6  methods:{ update(){ this.name = \u0026#39;updated name\u0026#39;; this.show = false; } },   and after clicking the button, beforeDestroy() and destroyed() on the HelloWorld component will be called.\n","description":"Notes on vue lifecycle, hooks, and a small example to demo what happens at which stage","id":4,"section":"posts","tags":["vue"],"title":"Vue Lifecycle","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/vue-lifecycle/"},{"content":"What are the key differences between block and object storage? I found this comparison and found it very helpful:\n    Block Storage Object Storage     Performance Performs best for big content and high stream throughput Strong performance with database and transactional data   Geography Data can be stored across multiple regions The greater the distance between storage and application, the higher the latency   Scalability Can scale infinitely to petabytes and beyond Addressing requirements limit scalability   Analytics Customizable metadata allows data to be easily organized and retrieved No metadata    Block storage means saving data in blocks, or raw storage volumes. Blocks can be seen as fixe-sized chunks. Programs only uses address to identify the blocks, and organize them to form the complete file. There’s no metadata associated with blocks. Therefore performance is faster when application and storage are local. However when they are farther apart, latency can be greater.\nAnother way of understanding block storage devices is that each block can be treated like a disk drive controlled by an external server operating system. Block storage is the most commonly used storage type for most applications and it can be either locally or network-attached(example: Amazon’s Elastic Block Storage service). Its typical usages are databases, any application which requires service side processing(like JAVA, PHP, and .Net), and mission-critical applications which all require low-latency operations.\nObject Storage means bundling data with customizable metadata tags and a unique identifier to form objects, which are stored in a flat address space(memory is treated as a single contiguous block with a single integer offset, starting from 0 to the maximum address).\nData kept on object storage devices, unlike data stored in block storage devices which can only be accessible when a block is mounted onto an OS, can be accessed directly through APIs. This guarantees data will not be lost and highly scalable. Its typical use cases are unstructured data like music, image, and video files, backup files, large data sets, and archive files. One example of object storage in the cloud is Amazon S3.\nWhat are the key problems a Data Lake solve? A data lake is a collection of all the data sources that a company collected about their customer, their transactions, operations and more. Back in the days companies may have separate data sources for different data, and people need to hop here and there to find the things that they are looking for. Data lakes solves this problem by bundle all the data together so that you can search and analyze things all in one place.\nData lakes use a structure without schema to organize data sources. Unique identifiers and meta tags are usually added to data so that you can easily find what you need, and different formats of data can be left as they are.\nIt is worth mentioning the differences between data lake and data warehouse to better understand the problem. Data in a data warehouse are highly organized and aren’t in their original form, but transformed in some pre-defined structure in the warehouse. This characteristic helps a data warehouse to solve a specific type of problem, but it may not be suitable to solve others. Data lakes however can be applied to various problems and its lack of tight structure make it versatile and flexible.\nWhat are data tags and how are they achieved? I believe you can setup a automated process of adding tags of basic info(data source, type, time, size, schema, etc), profiling info, across datasets info(primary key, foreign key, potential significant overlap), and business tags everytime new data is ingested into the data lake. As of how to achive that there\u0026rsquo;s many data discovery tools and code packages that can extract data tags.\nReferences https://cloudian.com/blog/object-storage-vs-block-storage/\nhttps://www.enterprisestorageforum.com/storage-technology/object-storage-vs-block-storage.html\nhttps://cloudacademy.com/blog/object-storage-block-storage/\nhttps://www.datameer.com/blog/whats-data-lakes-five-questions-answered/\nhttps://www.persistent.com/blogs/tags-first-read-fast-step-to-discover-explore-and-enrich-your-data-lake/\n","description":"course notes on different storage and data lakes.","id":5,"section":"posts","tags":["data lakes","cloud storage"],"title":"Cloud Analysis Course Notes - Cloud Storage and Data Lakes","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw8-datalake/"},{"content":"What are the problems with a “one size fits all” approach to relational databases? In the past, the reason why so many businesses model their data to try and fit the use cases of a relational database is that there is no other choice, so in reality many kinds of data are actually not suitable for relational model. Of course this is not to say relational databases cannot provide scalable, available and high performance service, it’s just that we now have many other database models to better suit different use cases. Here is a list of purposes for various types of databases:\nRelational: Developers rely on the functionality of the relational database to enforce the schema and preserve the referential integrity of the data within the database. Typical use cases for a relational database include web and mobile applications, enterprise applications, and online gaming.\nKey-value: Key-value databases are highly partitionable and allow horizontal scaling(adding more machines to the resource pool) at levels that other types of databases cannot achieve. Use cases such as gaming, ad tech, and IoT lend themselves particularly well to the key-value data model where the access patterns require low-latency Gets/Puts for known key values.\nDocument: Data is usually saved as JSON documents. Developers can persist data using the same document model format that they use in their application code.\nGraph: A graph database\u0026rsquo;s purpose is to make it easy to build and run applications that work with highly connected datasets. It’s best used to reflect the relationships between the nodes in graphs. Typical use cases for a graph database include social networking, recommendation engines, fraud detection, and knowledge graphs.\nIn-memory: Financial services, Ecommerce, web, and mobile application have use cases such as leaderboards, session stores, and real-time analytics that require microsecond response times and can have large spikes in traffic coming at any time.\nHow could a service like Google BigQuery change the way you deal with Data? BigQuery is a query service that allows you to run SQL-like queries against multiple terabytes of data in a matter of seconds. Is serverless, geared toward real-time analytics or big data analytics, automated backup and restore in the case something bad happens, gives developers access via REST API and SDKs including access via SQL, offers high availability so there is minimal to no downtime, has flexible pricing model where you only page for what you use and allows the use of standard SQL dialect so developers can transition from relational database to BigQuery without much learning difficulties.\nWhat problem does a \u0026ldquo;serverless\u0026rdquo; database like Athena solve? Athena offers teams a serverless front-end SQL query engine for an ETL or ELT process to an AWS S3 data lake. It is serverless, this means no infrastructure to manage. This also means you on ly pay for the queries you run, which will minimize costs. It is not a database service so you don’t have to go through the hassle of setting things up and managing the infrastructures.\nReferences https://www.allthingsdistributed.com/2018/06/purpose-built-databases-in-aws.html\nhttps://blog.openbridge.com/what-is-google-bigquery-5598778e5aeb\n","description":"Cloud Analysis course discussion note on ASICs and machine learning","id":6,"section":"posts","tags":["cloud databases","cloud computing","aws"],"title":"Cloud Analysis course notes - Cloud Databases","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw7-clouddb/"},{"content":"Docker\u0026rsquo;s two modes Interactive\nusage example: run jupyter, add python2.7 to test\nBackground\nusage example: run as server(running forever), website\nDocker is basically a better version of virtualenv, where you have an seperate environment (and a set of package versions) for every container\nDocker workflow example Here are the steps I did to containerize a simple flask app.\n Create Github repo Create ssh keys and upload to Github Git clone Create a local python virtual environment and source: python 3 -m venv ~/.dockerproj \u0026amp;\u0026amp; source ~/.dockerproj/bin/activate Create files:  Dockerfile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  FROMpython:3.7.3-stretch# Working DirectoryWORKDIR/app# Copy source code to working directoryCOPY . app.py /app/# Install packages from requirements.txt# hadolint ignore=DL3013RUN pip install --upgrade pip \u0026amp;\u0026amp;\\  pip install --trusted-host pypi.python.org -r requirements.txt#Expose port 8080 on the docker image#to make the flask website accessibleEXPOSE8080# Run app.py at container launchCMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;]  requirements.txt\npylint flask pandas Makefile\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  setup: python3 -m venv ~/.dockerproj install: pip install --upgrade pip \u0026amp;\u0026amp;\\ \tpip install -r requirements.txt test: #python -m pytest -vv --cov=myrepolib tests/*.py #python -m pytest --nbval notebook.ipynb validate-circleci: # See https://circleci.com/docs/2.0/local-cli/#processing-a-config circleci config process .circleci/config.yml lint: hadolint Dockerfile pylint --disable=R,C,W1203 app.py all: install lint test   app.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  #!/usr/bin/env python from flask import Flask from flask import jsonify from pandas import pandas as pd # If `entrypoint` is not defined in app.yaml, App Engine will look for an app # called `app` in `main.py`. app = Flask(__name__) @app.route(\u0026#39;/greeting\u0026#39;) def hello(): \u0026#34;\u0026#34;\u0026#34;Return a friendly HTTP greeting.\u0026#34;\u0026#34;\u0026#34; return \u0026#39;Hello Fellas continuous deployment on GCP rox yeeeeeheee!\u0026#39; @app.route(\u0026#39;/name/\u0026lt;value\u0026gt;\u0026#39;) def name(value): val = {\u0026#34;value\u0026#34;: value} return jsonify(val) @app.route(\u0026#39;/\u0026#39;) def html(): \u0026#34;\u0026#34;\u0026#34;Returns some custom HTML\u0026#34;\u0026#34;\u0026#34; return \u0026#34;\u0026#34;\u0026#34;\u0026lt;title\u0026gt;This is a Hello World World Page\u0026lt;/title\u0026gt;\u0026lt;h3\u0026gt;Hello\u0026lt;/h3\u0026gt;\u0026lt;br\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;This is a simple flask web page\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Wrapped inside a Docker container and running on cloud9\u0026lt;/p\u0026gt;\u0026#34;\u0026#34;\u0026#34; @app.route(\u0026#39;/pandas\u0026#39;) def pandas_sugar(): df = pd.read_csv(\u0026#34;https://raw.githubusercontent.com/noahgift/sugar/master/data/education_sugar_cdc_2003.csv\u0026#34;) return jsonify(df.to_dict()) if __name__ == \u0026#39;__main__\u0026#39;: # bind to 0.0.0.0 so that the outside world can access the docker container app.run(host=\u0026#39;0.0.0.0\u0026#39;, port=8080, debug=True)    configure CircleCI\nCreate a folder named .circleci, and put config.yml inside.\n  config.yml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  # Python CircleCI 2.0 configuration file## Check https://circleci.com/docs/2.0/language-python/ for more details#version:2jobs:build:docker:# Use the same Docker base as the project- image:python:3.7.3-stretchworking_directory:~/reposteps:- checkout# Download and cache dependencies- restore_cache:keys:- v1-dependencies- # fallback to using the latest cache if no exact match is found- v1-dependencies- - run:name:installdependenciescommand:| python3 -m venv venv.venv/bin/activatemakeinstall# Install hadolintwget-O/bin/hadolinthttps://github.com/hadolint/hadolint/releases/download/v1.17.5/hadolint-Linux-x86_64\u0026amp;\u0026amp;\\chmod+x/bin/hadolint- save_cache:paths:- ./venvkey:v1-dependencies- # run lint!- run:name:runlintcommand:| . venv/bin/activatemakelint  Go to your CircleCI page and setup the project.\n Build ang tag the image\ncd into the directory containing Dockerfile and run:\ndocker build --tag=\u0026lt;your_tag_name\u0026gt; .\nreplace with your own tag name\n  Run container and test the app\ndocker run -p 8080:8080 \u0026lt;your_tag_name\u0026gt;\nthis will map port 8080 on the container to port 8080 on the hosting machine.\nMy container was running on an EC2 instance so I would have to allow port 8080 on the inbound rule of the security group controlling the instance. Then I can use the public DNS of that instance to access the flask website with a browser.\n  Setup DockerHub\nSign in to DockerHub and create a new repository, note down the repo path, which would look someting like this: xnie1970/simple_python_from_cloud9\ngo back and run:\ndocker image tag \u0026lt;your_tag_name\u0026gt; \u0026lt;your_docker_path\u0026gt;\ndocker image push \u0026lt;your_docker_path\u0026gt; \nthe first time you do these you have to login using DockerHub credentials.\n  Alternatively you can write a script to do the pushing:\npush_docker.sh\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #!/usr/bin/env bash # This tags and uploads an image to Docker Hub #Assumes this is built #docker build --tag=app . dockerpath=\u0026#34;xnie1970/simple_python_from_cloud9\u0026#34; # Authenticate \u0026amp; Tag echo \u0026#34;Docker ID and Image: $dockerpath\u0026#34; docker login \u0026amp;\u0026amp;\\  docker image tag app $dockerpath #after the first time you don\u0026#39;t need to enter your credentials again # Push Image docker image push $dockerpath   don\u0026rsquo;t forget to chmod +x push-docker.sh before running the script.\n Test pulling image from DockerHub\ndocker pull xnie1970/simple_python_from_cloud9 docker run -p 8080:8080 xnie1970/simple_python_from_cloud9:latest   ","description":"course notes on docker","id":7,"section":"posts","tags":["containers","docker"],"title":"Cloud Analysis course discussion notes - Docker","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/notes-on-docker/"},{"content":"Ingredients    Item Measure     1/4 inch thick chicken breast 1 1/2 lb   All-purpose flour 3 tbsp   Salt to taste   Pepper to taste   Butter 3 tbsp   Mushrooms 8 oz   Finely chopped shallots 1 medium shallot   Minced garlic 2 cloves   Chicken broth 2/3 cup   Marsala wine 2/3 cup   Heavy cream 2/3 cup   Chopped fresh thyme 2 tsp    Steps  Place flour, salt, pepper in a ziplock bag, add chicken and shake to coat Cook chicken with olive oil and 2 tbsp butter until just barely cooked through, set aside Add 1 tbsp butter to same pan, cook mushrooms Add shallots, garlic and salt, cook a bit Add broth, wine, heavy cream, thyme, salt and pepper, gently boil 10-15 mins(until thickened to about half) Add chicken back, simmer 2-3 mins   ","description":"An amazing italian-american dish that pairs well with either salad, mashed potatoes or rice","id":8,"section":"posts","tags":["savory recipe"],"title":"Chicken Marsala","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/chicken-marsala/"},{"content":"Ingredients Base\n   Item Measure     Almond flour 0.8 cup   Sweetener(swerve) 3 tbsp   Melted butter 3 tbsp   Vanilla extract 2/3 tsp   Eggs 1   Salt small pinch    Filling\n   Item Measure     Full fat cream cheese 16 oz   Sweetener(swerve) 2/3 cup   Eggs 1   Lemon 1/5 to juice   Vanilla extract 2/3 tsp    ~= 6 inch cake\nSteps  Preheat oven to 300F, bring eggs and cream cheese to room temperature  Crust base\n2. Mix dry and wet ingredients seperately, then combine together\n3. lightly grease a 6 inch cake pan and spread batter out in the pan\n4. bake for 15 mins\nCheese filling\n5. Add sweetener to cheese, use hand mixer to combine\n6. Add egg, lemon juice and vanilla extract, combine\n7. Take out baked crust, fill in cheese mixure, bake at 350F for 35 mins\n8. Once it cooled down, cover with tin foil and refrigerate for 5 hours\n ","description":"Low carb lemon-y cheesecake","id":9,"section":"posts","tags":["baking recipe"],"title":"Cheese cake","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/cheesecake/"},{"content":"Ingredients    Item Measure     Almond flour 1 1/2 cup   Baking powder 1 tbsp   Salt 1/4 tsp   Garlic powder some   Onion powder some   Eggs 2   Full fat sour cream 1/2 cup   Melted butter 4 tbsp   Shredded cheddar cheese 1/2 cup    ~= 12 regular muffins\nSteps  Preheat oven to 450F, grease muffin pan Mix together almond flour, salt, baking powder, garlic powder and onion powder In another bowl combine eggs, sour cream, butter, and mix well Mix wet and dry ingredients Add in cheese Add batter in muffin pan and bake for 10-11 mins, until top is light golden brown   ","description":"Low carb savory biscuits with almond flour","id":10,"section":"posts","tags":["baking recipe"],"title":"Almond Flour Biscuits","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/almond-flour-biscuits/"},{"content":"Ingredients    Item Measure     Almond flour 2 1/2 cup   Erythritol(swerve) 1/2 cup   Baking powder 1 1/2 tsp   Salt a small pinch   Melted Butter 1/4 cup   Milk(/oat milk) 1/3 cup   Eggs 3   Vanilla extract 1/2 tsp   Blueberries 3/4 cup    ~= 6 large muffins\nSteps  Preheat oven to 350F, line or grease muffin pan Mix almond flour, sweetener, baking powder and salt Mix in melted butter, milk, eggs, and vanilla extract Add in blueberries Put batter in muffin pan, bake 20-25 min, until top is lightly golden brown.   ","description":"Low carb blueberry muffin with almond flour","id":11,"section":"posts","tags":["dessert recipe"],"title":"Blueberry Muffin","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/blueberry-muffin/"},{"content":"How could ASICs play an important role in Machine Learning going forward? Machine learning tasks, including the training and the inference, usually cost a lot of computational resources and time. So there is a big need for a hardware acceleration. Hardware acceleration is when dedicated hardware, usually in the form of a GPU today, is used to speed up the computing processes present in an AI workflow. Accelerators have many optimisations that make them suitable for training and executing an AI model, and here are some of them:\nParallelism: ML is inherently parallel workload, so it would be one of the biggest requirement for hardware acceleration\nLow-Precision Arithmetic: ML jobs usually works with floating-point numbers, and reducing the precision or ‘detail’ of floating-point operations provides an easy way to increase the effectiveness of the hardware.\nAdvanced Low-Level Architecture: ML-specific chips usually implemented new ways of organizing physical architecture on the chips to achieve better optimization for deep learning applications. An example would be google’s Tensor Processing Unit(TPU). It brings memory and CPU unit closer together to better perform TensorFlow tasks.\nAllowances For In-Memory Processing: The accelerator will also need to make allowances for the smooth execution of in-memory analytics, which speeds up the training by increasing access to the dataset. This includes a high-speed interconnect between the processing unit and the memory (currently HBM2), along with a larger pool of memory to fit big datasets.\nApplication Specific Integrated Circuits, or ASICs, are chips that are designed and manufactured for a specific purpose, task, or application. As opposed to FPGA(Field Programmable Gate Arrays) which are rewritable and reprogrammable, ASICs are permanent and cannot be modified. ASICs do tend to have the best efficiency, performance, as well as power as compared to FPGAs. Although FPGAs are flexible, they can be quite difficult to make and expensive as well. Also they are not as good in terms of performance comparing to GPUs and ASICs. Of course, CPUs can also be used to train and execute ML models, they tend to provide less performance power than optimized hardware chips. GPUs are flexible and fast, their underlying matrix operations and scientific algorithms makes them ideal for graphics. With ASICs, you get the best of all worlds as it is basically a customizable chip that can be designed to accomplish a very specific task at high power, efficiency, and performance.\nReferences https://anysilicon.com/introduction-to-artificial-intelligence-in-asics/\nhttps://analyticsindiamag.com/why-asics-are-becoming-so-widely-popular-for-ai/\n","description":"Cloud Analysis course discussion note on ASICs and machine learning","id":12,"section":"posts","tags":["machine learning","asics"],"title":"Cloud Analysis course discussion note - ASICs","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw6-asics/"},{"content":"How does the CAP Theorem play a role in designing for the cloud? The CAP Theorem says that for a distributed system, you can only have 2 of the 3 things: Consistency, Availability and Partition Tolerance.\nConsistency means that only have one copy of up-to-date data, and that all clients see the same data at the same time, no matter which node they connect to. For this to happen, whenever data is written to one node, it must be instantly forwarded or replicated to all the other nodes in the system before the write is deemed ‘successful.’\nAvailability means data is always available for requests from client even when one or more nodes are down. Another way to state this—all working nodes in the distributed system return a valid response for any request, without exception.\nA partition is basically a network break, or a communication delay/lost between nodes within a distributed system. Partition Tolerance is a system’s ability to still work despite any number of network breakdowns between the nodes.\nAny cloud system is a distributed system, so to design a cloud system we always have to have CAP theorem in mind. Of course, if no partition exists, one system can have both consistency and availability, but there’s always going to be times when things go wrong, so we have to make a choice between C and A. what’s the ultimate design goal in a distributed system for maximum data consistency and 100% availability – it’s design for failure.\nWhat are the implications of Amdahl’s law for Machine Learning projects? Amdahl’s law gives the following formula to measure the speedup of running sub-tasks in parallel (over different processors) versus running them sequentially (on a single processor):\n$$ S_L (s) = \\cfrac{1}{(1-p) + \\cfrac{p}{s}} $$\nwhere s is the speedup of the part of the task that benefits from improved system resources, and p is the proportion of execution time that the part benefiting from improved resources originally occupied(parallel portion). So the theoretical time benefit from doing things parallel is largely dependent on how much of the program is made parallel, which is p.\nEven you can achieve perfect parallelism and scale up the program to match the theoretical estimation, the more cores you have, the less it’s going to improve the performance. There’s always overhead with parallelism at the hardware level(additional computer cycles required to divide tasks into subtasks and compile results after). It’s going to grow the more you parallelize, and it also applies for parallelism in a cloud environment.\nIn conclusion, even though it’s tempting to parallelize large data set in machine learning project, you should always keep in mind that parallelism wouldn’t necessarily lead to performance improvement.\nReferences https://www.ibm.com/cloud/learn/cap-theorem\nhttps://en.wikipedia.org/wiki/Amdahl%27s_law\nhttps://www.kdnuggets.com/2017/04/must-know-parallelism-algorithms.html\n","description":"Cloud Analysis course discussion note on CAP Theorem and Amdahl's Law, and their relations with machine learning projects","id":13,"section":"posts","tags":["cloud computing","machine learning"],"title":"Cloud Analysis course notes - CAP Theorem and Amdahl's Law","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw5-cap/"},{"content":"What are containers? The concept of containers is similar to that of virtual machines. The differentiating factor is that VMs virtualize at the hardware level and containers virtualize at the operating system level. The containerization approach creates a more lightweight and flexible environment by allowing applications to share an operating system while maintaining their own executables, code, libraries, tools, and configuration files.\nA container consists of an entire runtime environment: an application, plus all its dependencies, libraries and other binaries, and configuration files needed to run it, bundled into one package. By containerizing the application platform and its dependencies, differences in OS distributions and underlying infrastructure are abstracted away.\nWhat problem do containers solve? Let’s look at this problem by comparing containers with VMs. Comparing to VMs, containers offers the following benefits:\n Lightweight  A container may be only tens of megabytes in size, whereas a virtual machine with its own entire operating system may be several gigabytes in size. Because of this, a single server can host far more containers than virtual machines.\n Just-in-time  Virtual machines may take several minutes to boot up their operating systems and begin running the applications they host, while containerized applications can be started almost instantly. That means containers can be instantiated in a \u0026ldquo;just in time\u0026rdquo; fashion when they are needed and can disappear when they are no longer required, freeing up resources on their hosts.\n Modularity  Rather than run an entire complex application inside a single container, the application can be split in to modules (such as the database, the application front end, and so on). This is the so-called microservices approach. Applications built in this way are easier to manage because each module is relatively simple, and changes can be made to modules without having to rebuild the entire application.\n Scalable  Since they have all of the above advantages, containers can be easily scaled up. You can basically run 1000 environments at the same time at a click of your fingers.\n","description":"Cloud Analysis course discussion note on containers","id":14,"section":"posts","tags":["cloud computing","containers"],"title":"Cloud Analysis course notes - Containers","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw4-containers/"},{"content":"What are the different layers of network security on AWS and what unique problems do each solve? According to the AWS security white paper, AWS has the following layers of network security:\n Secure Network Architecture  There are network devices like firewalls at the edge of the aws network to closely monitor and guard the information flow in both ways. ACLs, or Access Control List are in place to manage the traffic flow.\n Secure Access Points  AWS has a limited number of access points(API endpoints) to the cloud to monitor more comprehensively of inbound and outbound traffic. They allow HTTPS access which we can use to establish secure connection with our storage or compute instances within AWS.\n Transmission Protection  SSL protocal(Secure Sockets Layer) can be utilized to connect to AWS access point and prevent eavesdropping, tampering and message forgery. In addition, Amazon Virtual Private Cloud (VPC) offers a private subnet within the AWS cloud, and VPN can also be used to establish an encrypted tunnel between the Amazon VPC and your data center.\n Amazon Corporate Segregation  The AWS Production network is segregated from the Amazon Corporate network. Only approved personnel, developers and administrators, can access the AWS corporate network through a bastion host that restricts access to cloud components which requires a SSH key, and all activities will be logged for security review.\n Fault-Tolerant Design  Amazon’s infrastructure has a high level of availability. Data centers are built in clusters in various global regions. If failure happens, customer data will be automatically moved away from the affected area. We can also place instances and store data within multiple geographic regions/availability zones to provide more physical separation and better availability.\n Network Monitoring and Protection  There are a wide variety of automated monitoring systems to detect unusual or unauthorized activities and conditions at incoming and outgoing communication points. Alarms are configured to notify operations and management personnel when thresholds are crossed on key operational metrics. On top of that, there are always people on call to handle those alarms or issues regarding operation. Ticketing system is in place to propagate severe/emergence incident to upper management. There are also documentation of aid and help personnel to handle incidents.\nWhat problem do AWS Spot instances solve and how could you use them in your projects? One of the problems with the on-demand business model (like normal EC2 instances) is that at any given time, there are likely to be compute resources that are not being utilized. These resources represent hardware capacity that AWS has paid for but are sitting idle. Rather than allowing these computing resources to go to waste, AWS offers them at a substantially discounted rate, with the understanding that if someone needs those resources for running a normal EC2 instance, that instance will take priority over spot instances and the spot instance may be interrupted at any time.\nThose who wish to use spot resources simply tell AWS how much they are willing to pay per hour for those resources. Spot instances will continue to run until the cost of the resource meets or exceeds the agreed-upon price. Of course, subscribers are also free to stop or terminate spot instances themselves.\nSince spot instances are subject to sudden interruptions, they should not be used on situations where your applications need to be up and running at all times.\nHowever, in our project, if we want to do an experiment or prototype, anything temporary, we can request a spot instance and utilize large computational power at a much lower price.\n","description":"Cloud Analysis course discussion note on AWS network security strategy and spot instance","id":15,"section":"posts","tags":["cloud computing","aws"],"title":"Cloud Analysis course notes - AWS","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw3-aws/"},{"content":"What is IAC and what problem does it solve? IAC stands for Infrastructure as Code. is a method to provision and manage IT infrastructure through the use of source code, rather than through standard operating procedures and manual processes.\nYou’re basically treating your servers, databases, networks, and other infrastructure like software. And this code can help you configure and deploy these infrastructure components quickly and consistently.\nIaC helps you automate the infrastructure deployment process in a repeatable, consistent manner, which has many benefits.\nWith IaC, deployment doesn’t always have to all fall on DevOps’ shoulders. Developers today can write not only application code, but also the infrastructure to run applications. And companies today can even integrate infrastructure code into the software development system to vastly increase development efficiency.\nIaC let machines do all the work to set up cloud environment and manage resources that you need, which makes it faster and more efficient than humans would do.\nIt also makes continuously testing systems and processes possible for infrastructure configuration. In modern software systems, you can build a \u0026ldquo;deployment pipeline\u0026rdquo; for infrastructure code, which allows you to practice continuous delivery processes for infrastructure changes.\nIf infrastructure is being turned into code, the biggest perk should be version control. With version control it would be easier to roll back to a working configuration if anything goes wrong, and the environment would be easier to track and maintain, errors can be found and fixed more quickly.\nHow should a company decide on what level of cloud abstraction to use SaaS Software as a Service. SaaS utilizes the internet to deliver applications, which are managed by a third-party vendor, to its users. A majority of SaaS applications run directly through your web browser, which means they do not require any downloads or installations on the client side. Use it if you are after features that you don’t want to build yourself and that can significantly improve your product\nPaaS Platform as a Service. Example: Google Kubernetes Engine, Azure Container Service. PaaS delivers a framework for developers that they can build upon and use to create customized applications.Use it if you want to just focus on the application code without having to worry about infrastructure.\nIaaS Infrastructure as a Service. Example: AWS EC2, AWS VPC, AWS ELB. IaaS is fully self-service for accessing and monitoring computers, networking, storage, and other services. IaaS allows businesses to purchase resources on-demand and as-needed instead of having to buy hardware outright. Use it if you want granular control over your product’s infrastructure.\nMaaS Metal(Machine) as a Service. Use if you want just the machine and the freedom to install OS and do configurations yourself. This is particularly useful for multi-nodes applications like Hadoop clusters.\nServerless It’s basically FaaS(Function as a Service, like AWS Lambda)+BaaS(Backend as a Service, like AWS DynamoDB, Google App Engine). It’s similar to Paas, use it if you want to focus on code, but Serverless provides even more deployment ease and even less configuration.\n","description":"Cloud Analysis course discussion note on IAC, SaaS, PaaS, IaaS, MaaS and Serverless","id":16,"section":"posts","tags":["cloud computing","IaC"],"title":"Cloud Analysis course note - IaC and cloud service model","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hw2-iac/"},{"content":"This is the basic workflow of this blog and list of steps to continuous deployment. It is also a course note and the majority of the materials came from my professor\u0026rsquo;s post here\nSet up Hugo workflow using AWS Cloud9 and Github First we are going to setup a cloud9 environment as the working directory of the whole thing.\nCloud9 provides a quick and easy linux environment/web-based editor hosted on an EC2 instance that would be automatically stopped after 30m of unuse.\nIt\u0026rsquo;s quite cost effective and convenient especially for windows users.\n  Launch an AWS Cloud9 Environment   Log into your AWS account and go to the Cloud9 console. Create a new environment with the free tier instance and the default configurations.\n  Install Hugo   Use wget to downlad the latest version of hugo binary. You can find the latest release here.\nwget https://github.com/gohugoio/hugo/releases/download/v\u0026lt;LATEST_VERSION\u0026gt;/hugo_\u0026lt;LATEST_VERSION\u0026gt;_extended_Linux-32bit.tar.gz Note that here we used the extended version since we are going to use a theme that depends on the extended version of hugo.\nNow unzip and install the hugo binary:\ntar xzvf hugo_0.64.0_extended_Linux-32bit.tar.gz mkdir -p ~/bin mv ~/environment/hugo ~/bin #assuming that you download this into ~/environment which hugo #this shows the `path` to hugo and should be '~/bin/hugo' hugo version #check that hugo is successfully installed   Create a website   You might have seen a lot of people saying that hugo is better and faster than jekyll, and part of the reason is that hugo is just a go binary.\nBecause of that, development and deployment is very simple and quick with hugo.\nNow you can use hugo new site mywebsite to create a new site.\nHugo has a wide range of theme selections and in my personal experience the best way to use themes is:\n Fork theme repo into your github account  This blog used hugo-theme-zzo\nAdd the theme into your website as a submodule  cd mywebsite git submodule add https://github.com/\u0026lt;YOUR_GITHUB_ID\u0026gt;/hugo-theme-zzo themes/zzo Make your own changes to the theme and track it into your forked repo Update the main site repo to the latest theme repo version(we will be tackling this later) Add the theme into the config file  echo 'theme = \u0026quot;zzo\u0026quot;' \u0026gt;\u0026gt; config.toml\ngit pull changed and update from the original theme repo if necessary(may need to fix conflict)  Now create a new post:\nhugo new posts/my-first-post.md\nThis will create a new markdown file with the created timestamp. You can edit this file with the cloud9 editor.\nNote that if you use this command to generate a new post, the draft field will be defaulted to true which means this file will not be built into html and it will not be visible on the deployed website.\n  Run Hugo locally in Cloud9   Here we are going to run hugo as a deployment server. To access the port after the server has started, we will need to open up a port on the EC2 security groups.\nJust look for the security group tab on the EC2 console page, and look for the one with the same name as your current Cloud9 environment:\nGo to the Inbound tab, click the Edit button and add a new Custom TCP Rule with port 8080.\nNow we can go back to Cloud9 and use curl ipinfo.io to find out the public ip address. The output should look something like this:\n1 2 3 4 5 6 7 8 9 10 11 12  { \u0026#34;ip\u0026#34;: \u0026#34;54.80.213.10\u0026#34;, #this is the address we want \u0026#34;hostname\u0026#34;: \u0026#34;ec2-54-80-213-10.compute-1.amazonaws.com\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Virginia Beach\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;Virginia\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;loc\u0026#34;: \u0026#34;36.8512,-76.1692\u0026#34;, \u0026#34;org\u0026#34;: \u0026#34;AS14618 Amazon.com, Inc.\u0026#34;, \u0026#34;postal\u0026#34;: \u0026#34;23465\u0026#34;, \u0026#34;timezone\u0026#34;: \u0026#34;America/New_York\u0026#34;, \u0026#34;readme\u0026#34;: \u0026#34;https://ipinfo.io/missingauth\u0026#34; }   Run hugo with the following options. This will start the server and left it running.\nhugo serve --bind=0.0.0.0 --port=8080 --baseURL=http://\u0026lt;YOUR_PUBLIC_IP_ADDRESS\u0026gt;/\nOpen up a new tab in your browser and go to http://\u0026lt;YOUR_PUBLIC_IP_ADDRESS\u0026gt;:8080/ to preview your site. Cool thing about this is you can edit your markdown file and it will render and reflect the change on the site in real time.\nHost the static site on S3 First follow this instruction to set up an S3 bucket for static website hosting.\nYou should also set a bucket policy as below from the Permissions tab on your bucket page, with your own bucket name:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::\u0026lt;YOUR_BUCKET_NAME\u0026gt;/*\u0026#34; ] } ] }     Deploy website manually to S3   Before going fully automatic, it\u0026rsquo;s best to first deploy to your bucket manually and see if everything works out.\nAdd the following to the existing config.toml file(overwrite fields if exists)\n1 2 3 4 5 6 7 8 9  baseURL = \u0026#34;http://\u0026lt;YOUR_BUCKET_NAME\u0026gt;.s3-website-\u0026lt;YOUR_BUCKET_REGION\u0026gt;.amazonaws.com\u0026#34; title = \u0026#34;\u0026lt;YOUR_TITLE\u0026gt;\u0026#34; theme = \u0026#34;zzo\u0026#34; [[deployment.targets]] # An arbitrary name for this target. name = \u0026#34;\u0026lt;WHATEVER_NAME\u0026gt;\u0026#34; URL = \u0026#34;s3://\u0026lt;YOUR_BUCKET_NAME\u0026gt;/?region=\u0026lt;YOUR_BUCKET_REGION\u0026gt;\u0026#34; #your bucket here   Now you can deploy using the following commands:\nhugo hugo deploy If everything works out you should see something like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  ec2-user:~/environment/quickstart (master) $ hugo | EN -------------------+------ Pages | 34 Paginator pages | 0 Non-page files | 0 Static files | 105 Processed images | 0 Aliases | 9 Sitemaps | 1 Cleaned | 0 Total in 5692 ms ec2-user:~/environment/quickstart (master) $ hugo deploy Deploying to target \u0026#34;jan23awsbucket\u0026#34; (s3://jan23-website-hosting/?region=us-east-1) Identified 1 file(s) to upload, totaling 244 kB, and 0 file(s) to delete. Success!   Now your website should be accessible through the static website hosting endpoint.\nContinuous Delivery with AWS CodeBuild   Check code into Github    Create a new repo and add .gitignore with Go (Optional) Add public to .gitignore to skip pushing all built html files to repo Create a Makefile to remove the public folder:  1 2 3  clean: echo \u0026#34;deleting generated html...\u0026#34; rm -rf public   Now run make clean to remove all the generated html files to prevent them from being uploaded to version control.\nAdd repo as a remote and push code  git remote add origin git@github.com:\u0026lt;github_username\u0026gt;/my_hugo_site.git git status git add * git pull --allow-unrelated-histories origin master git branch --set-upstream-to=origin/master git push   Setup project on AWS CodeBuild   Go to AWS CodeBuild and create a new project in the same region as your S3 bucket\nNote in the Source section, use OAuth to login to Github and choose Repository in my Github Account. In Additional Configuration, choose git clone depth = 1, tick Use Git submodules and Report build statuses to source provider when your builds start and finish\nIn the Webhook section, tick Rebuild every time a code change is pushed to this repository.\nThe Environment section should look something like this:\nNow go to the Service role page from the project detail page, and attach an AdministratorAccess policy to that role.\nOk, one last step. Go back to Cloud9 and add a buildspec.yml file to tell AWS what you want to do with the automatic build:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  version:0.2environment_variables:plaintext:HUGO_VERSION:\u0026#34;0.64.0\u0026#34;phases:install:runtime-versions:docker:18commands:- cd/tmp- wgethttps://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_${HUGO_VERSION}_extended_Linux-64bit.tar.gz- tar-xzfhugo_${HUGO_VERSION}_extended_Linux-64bit.tar.gz- mvhugo/usr/bin/hugo- cd- - rm-rf/tmp/*build:commands:- rm-rfpublic- hugo- awss3syncpublic/s3://\u0026lt;YOUR_BUCKET_NAME\u0026gt;/--region\u0026lt;YOUR_BUCKET_REGION\u0026gt;--deletepost_build:commands:- echoBuildcompletedon`date`  DONE! Now you can push something to your github repo and check out the new build that pops up on the build history page of your CodeBuild project.\n","description":"Steps and notes to build a continuous deployment workflow of Hugo website with the help of AWS S3 and CodeBuild","id":17,"section":"posts","tags":["cloud computing","hugo","aws","continuous deployment"],"title":"Hosting Hugo Website on AWS S3 with Continuous Deployment","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/hugo-on-s3-with-cd/"},{"content":"The git revert command can be considered an \u0026lsquo;undo\u0026rsquo; type command, however, it is not a traditional undo operation. Instead of removing the commit from the project history, it figures out how to invert the changes introduced by the commit and appends a new commit with the resulting inverse content. This prevents Git from losing history, which is important for the integrity of your revision history and for reliable collaboration.\nSyntax git revert \u0026lt;COMMIT_ID\u0026gt; Here COMMIT_ID can be any form that git recognizes, from regular 40-digit, short SHA-1, to something like \u0026lsquo;HEAD\u0026rsquo;.\nThis means any change done by \u0026lt;COMMIT_ID\u0026gt; will be reverted and a new commit is created. You will be prompted to enter a regular commit message and after that you can do git push normally.\ngit revert \u0026lt;COMMIT_ID\u0026gt;..HEAD Reverts all the changes done by the commits in between, or similarly you can also do\ngit revert \u0026lt;COMMIT_ID1\u0026gt;..\u0026lt;COMMIT_ID2\u0026gt; Compare to other \u0026lsquo;undo\u0026rsquo; methods git reset This is like a complete rollback and it would move the working tree back to the last commited state. No new commit will be created and commits after the designated target may be lost. And it will change the commit history.\nYou can have several options as to what to do with your index(set of files that\u0026rsquo;s going to be the next commit) and the working directory:\ngit reset --soft will only change the current HEAD to point to another commit, but the index and working directory remains unchanged.\ngit reset --mixed will change the HEAD pointer and the index, but the working directory will stay the same.\ngit reset --hard will change everything. This means every change after the other commit will be gone forever.\n(However if you accidentally erased something with the \u0026ndash;hard option there\u0026rsquo;s always git reflog to help you get almost everything back)\ngit checkout This is used to change the HEAD pointer to another commit or switch between branches. It will also rollback file changes after that commit and can change the files in the working directory. The commit history will not be changed.\nTypical use cases:\n undo local changes to a file: git checkout file_name undo local commits:  git checkout target_branch git reset HEAD~2 #rollback 2 commits Git cheat sheet ","description":"Git revert and its comparison to reset","id":18,"section":"posts","tags":["git"],"title":"Git Revert","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/git-revert/"},{"content":"This is a course note and the majority of the meterials came from this qwiklab\nOverview Google Kubernetes Engine (GKE) provides a managed environment for deploying, managing, and scaling your containerized applications using Google infrastructure. The Kubernetes Engine environment consists of multiple machines (specifically Google Compute Engine instances) grouped together to form a container cluster.\nKubernetes Engine clusters are powered by the Kubernetes open source cluster management system. Kubernetes provides the mechanisms through which you interact with your container cluster. You use Kubernetes commands and resources to deploy and manage your applications, perform administration tasks and set policies, and monitor the health of your deployed workloads.\nKey components and concepts Pods Pods represent and hold a collection of one or more containers. Generally, if you have multiple containers with a hard dependency on each other, you package the containers inside a single pod.\nPods also have volumes attached to them and containers within a pod can share this attached volume through a shared namespace and communicate with each other.\nOne pod had one shared network namespace which means there\u0026rsquo;s one ip address per pod.\nServices Services provide stable endpoints for Pods. They are there to help you communicate with a set of pods, since pods don\u0026rsquo;t tend to be persistent and they sometimes would stop or restart due to reasons like failed liveness or readiness checks, and that would result in different ip addresses.\nServices used labels to point to pods. So all you have to do to add endpoints to a service is giving a label to each pod inside and the service will pick them up and expose them.\nThere are three types of services:\n ClusterIP (internal) \u0026ndash; the default type means that this Service is only visible inside of the cluster NodePort \u0026ndash; gives each node in the cluster an externally accessible IP LoadBalancer \u0026ndash; adds a load balancer from the cloud provider which forwards traffic from the service to Nodes within it.  Deployments Deployments are there to make sure that there are always that amount of pods(replicas) that you want there to be. Deployments use Replica Sets to manage starting and stopping the Pods.\nAnother cool thing about deployments is that it can recreate and start pods on another node automatically if the node that some pods are residing suddenly failed, thus make the pods inside highly available.\nkubectl commands List of commands and example usage can be found on the above qwiklab.\n","description":"Kubernetes and its orchestration basics","id":19,"section":"posts","tags":["cloud computing","containers"],"title":"Kubernetes basics","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/kubernetes/"},{"content":"Nutty Cloud Souffle this is the recipe for my easy souffle\n","description":"","id":22,"section":"posts","tags":null,"title":"Another Post","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/another-post/"},{"content":"Test on continuous delivery with aws codebuild this is an update lalala\n","description":"","id":24,"section":"posts","tags":null,"title":"Test Post on CodeBuild","uri":"http://jan23-website-hosting.s3-website-us-east-1.amazonaws.com/posts/my-first-post/"}]